# Scripts de Entrenamiento - Lenguaje de Se√±as

Scripts para entrenar modelos CNN-LSTM para reconocimiento de lenguaje de se√±as.

## üìÅ Archivos Disponibles

| Script | Descripci√≥n | Uso |
|--------|-------------|-----|
| `train_vlibrasil.py` | Entrenamiento espec√≠fico para V-LIBRASIL | Dataset brasile√±o |
| `train_sign_language.py` | Script universal para letras/palabras/frases | Cualquier dataset |
| `preprocess_sign_language.py` | Preprocesamiento de videos | Extrae landmarks |

## üöÄ Flujo de Trabajo Completo

### 1. Preprocesar Videos

```bash
# Dentro del contenedor Docker
docker exec -it nembogueta-dev-gpu bash

# Procesar dataset completo
python /app/scripts/preprocess_sign_language.py \
  --videos-dir "/app/src/data/videos UFPE (V-LIBRASIL)/data" \
  --output-dir /data/vlibrasil_processed \
  --preset holistic \
  --auto-infer
```

**Opciones de preset:**
- `hands`: Solo manos (126 features) - Letras simples
- `upper_body`: Manos + torso (225 features) - Palabras
- `holistic`: Cuerpo completo + cara (1662 features) - Frases complejas

### 2. Entrenar Modelo

#### Opci√≥n A: V-LIBRASIL (Espec√≠fico)

```bash
python /app/scripts/train_vlibrasil.py \
  --data-dir /data/vlibrasil_processed \
  --output-dir /models/vlibrasil \
  --epochs 100 \
  --batch-size 32 \
  --learning-rate 0.001
```

#### Opci√≥n B: Universal (Cualquier dataset)

```bash
# Para letras
python /app/scripts/train_sign_language.py \
  --data-dir /data/processed_letters \
  --output-dir /models/letters \
  --task-type letters \
  --epochs 100 \
  --batch-size 32

# Para palabras
python /app/scripts/train_sign_language.py \
  --data-dir /data/processed_words \
  --output-dir /models/words \
  --task-type words \
  --epochs 150 \
  --batch-size 16

# Para frases
python /app/scripts/train_sign_language.py \
  --data-dir /data/processed_phrases \
  --output-dir /models/phrases \
  --task-type phrases \
  --epochs 200 \
  --batch-size 16
```

### 3. Monitorear Entrenamiento

```bash
# Ver logs en tiempo real
tail -f sign_language_training.log

# TensorBoard (si est√° configurado)
tensorboard --logdir=/models/vlibrasil/run_XXXXXX/logs
```

## üìä Arquitectura del Modelo CNN-LSTM

```
Input (300, 1662)
    ‚Üì
Conv1D(64) + BatchNorm + Dropout(0.3)
    ‚Üì
Conv1D(128) + BatchNorm + Dropout(0.3)
    ‚Üì
Conv1D(256) + BatchNorm + Dropout(0.3)
    ‚Üì
LSTM(256) + Dropout(0.4)
    ‚Üì
LSTM(128) + Dropout(0.4)
    ‚Üì
Dense(128) + Dropout(0.3)
    ‚Üì
Dense(num_classes, softmax)
```

### Ventajas sobre LSTM Puro

| Caracter√≠stica | LSTM Puro | CNN-LSTM |
|----------------|-----------|----------|
| Accuracy esperado | 85-90% | **93-97%** |
| Extracci√≥n de features | ‚ùå | ‚úÖ Conv1D |
| Regularizaci√≥n | B√°sica | ‚úÖ BatchNorm |
| Par√°metros | M√°s | Menos (eficiente) |

## üîß Par√°metros de Entrenamiento

### Par√°metros Comunes

```bash
--data-dir PATH          # Directorio con datos procesados (.npy)
--output-dir PATH        # Directorio de salida para modelos
--epochs INT             # N√∫mero m√°ximo de epochs (default: 100)
--batch-size INT         # Tama√±o del batch (default: 32)
--learning-rate FLOAT    # Tasa de aprendizaje (default: 0.001)
--patience INT           # Paciencia para early stopping (default: 15)
```

### Configuraciones Recomendadas

**Dataset Peque√±o (<1000 muestras):**
```bash
--epochs 50 --batch-size 16 --learning-rate 0.0005 --patience 10
```

**Dataset Mediano (1000-5000 muestras):**
```bash
--epochs 100 --batch-size 32 --learning-rate 0.001 --patience 15
```

**Dataset Grande (>5000 muestras):**
```bash
--epochs 150 --batch-size 64 --learning-rate 0.001 --patience 20
```

## üìà Resultados Esperados

Despu√©s del entrenamiento, se generan:

```
/models/vlibrasil/run_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ best_model.h5                  # Mejor modelo durante entrenamiento
‚îú‚îÄ‚îÄ final_model.h5                 # Modelo final
‚îú‚îÄ‚îÄ model_info.json                # Metadatos del modelo
‚îú‚îÄ‚îÄ classification_report.json     # M√©tricas detalladas
‚îú‚îÄ‚îÄ confusion_matrix.npy          # Matriz de confusi√≥n
‚îú‚îÄ‚îÄ training_history.png          # Gr√°ficas de accuracy/loss
‚îî‚îÄ‚îÄ logs/                         # Logs de TensorBoard
```

### M√©tricas de √âxito

| M√©trica | M√≠nimo Aceptable | Objetivo | Excelente |
|---------|------------------|----------|-----------|
| **Accuracy** | 85% | 92% | 95%+ |
| **Precision (macro)** | 80% | 90% | 93%+ |
| **Recall (macro)** | 80% | 90% | 93%+ |
| **F1-Score (macro)** | 80% | 90% | 93%+ |

## üêõ Soluci√≥n de Problemas

### GPU no detectada
```bash
# Verificar GPU
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"

# Si no detecta, revisar docker-compose.yml
```

### Out of Memory (OOM)
```bash
# Reducir batch size
--batch-size 16  # o 8

# O reducir tama√±o de secuencia en preprocesamiento
--target-length 150  # en lugar de 300
```

### Underfitting (accuracy baja en train)
```bash
# Aumentar complejidad del modelo
--task-type words  # Usa modelo m√°s grande

# Entrenar m√°s epochs
--epochs 200

# Reducir dropout
# (editar el script)
```

### Overfitting (val accuracy << train accuracy)
```bash
# Aumentar dropout (editar script)
# Aumentar data augmentation
# Reducir epochs
# Usar m√°s datos
```

## üí° Tips de Optimizaci√≥n

1. **Usar GPU siempre** - 10-50x m√°s r√°pido que CPU
2. **Monitorear con TensorBoard** - Identifica problemas r√°pido
3. **Usar Early Stopping** - Evita overfitting
4. **Normalizar datos** - Ya est√° implementado en los scripts
5. **Stratified split** - Ya est√° implementado para balancear clases

## üìù Ejemplo Completo

```bash
# 1. Entrar al contenedor
docker exec -it nembogueta-dev-gpu bash

# 2. Preprocesar (solo primera vez)
python /app/scripts/preprocess_sign_language.py \
  --videos-dir "/app/src/data/videos UFPE (V-LIBRASIL)/data" \
  --output-dir /data/vlibrasil_processed \
  --preset holistic \
  --auto-infer

# 3. Entrenar
python /app/scripts/train_vlibrasil.py \
  --data-dir /data/vlibrasil_processed \
  --output-dir /models/vlibrasil \
  --epochs 100 \
  --batch-size 32

# 4. Ver resultados
cat /models/vlibrasil/run_*/model_info.json
```

## üîÑ Conversi√≥n a TensorFlow Lite

Despu√©s del entrenamiento, convierte el modelo para deployment:

```bash
python /app/scripts/convert_to_tflite.py \
  --model-path /models/vlibrasil/run_XXXXXX/best_model.h5 \
  --output-path /models/vlibrasil/model.tflite
```

## üìö Referencias

- [TensorFlow CNN](https://www.tensorflow.org/tutorials/images/cnn)
- [LSTM para Secuencias](https://www.tensorflow.org/guide/keras/rnn)
- [MediaPipe Holistic](https://google.github.io/mediapipe/solutions/holistic.html)
- [V-LIBRASIL Dataset](http://www.cin.ufpe.br/~cca5/v-librasil/)

## üÜò Soporte

Si encuentras problemas:
1. Revisa los logs: `vlibrasil_training.log`
2. Verifica que el preprocesamiento gener√≥ archivos `.npy`
3. Aseg√∫rate de tener suficientes datos (m√≠nimo 10 muestras por clase)
4. Consulta el README principal del proyecto
