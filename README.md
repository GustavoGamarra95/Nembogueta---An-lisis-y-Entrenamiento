# √ëemongeta - Python Module

**Sistema de Reconocimiento de Lenguaje de Se√±as Paraguayo (LSPy)**
**M√≥dulo de An√°lisis y Entrenamiento**

## Descripci√≥n

El m√≥dulo `√ëemongeta - Python` contiene scripts para la recolecci√≥n, preprocesamiento, an√°lisis, entrenamiento y conversi√≥n de modelos CNN-LSTM para el reconocimiento de gestos en **Lenguaje de Se√±as Paraguayo (LSPy)**. Los modelos est√°n optimizados para alcanzar una precisi√≥n del 95% en las categor√≠as de letras (a-z, √±), palabras (ej. juicio, abogado) y frases (ej. acceso a la justicia).

### Enfoque Principal del Proyecto

Este proyecto est√° enfocado en el desarrollo de reconocimiento de lenguaje de se√±as para **Paraguay**, con soporte biling√ºe para:
- **Espa√±ol paraguayo**
- **Guaran√≠**

El sistema utiliza t√©cnicas de deep learning con arquitecturas CNN-LSTM para el reconocimiento en tiempo real de:

- **Alfabeto dactilol√≥gico** (A-Z, √ë): Reconocimiento de letras individuales
- **Handshapes (Formas de mano)**: Clasificaci√≥n de configuraciones manuales por orientaci√≥n
- **Palabras completas**: Reconocimiento de se√±as completas en LSPy
- **Traducci√≥n biling√ºe**: Conversi√≥n de texto Espa√±ol/Guaran√≠ a glosas LSPy
- **Expresiones faciales**: An√°lisis de componentes no manuales

### Trabajo con LIBRAS

Como parte del desarrollo y entrenamiento del sistema, se utiliza el dataset **V-LIBRASIL** (Lenguaje de Se√±as Brasile√±o) para:
- Desarrollo y prueba de arquitecturas de modelos
- Entrenamiento de modelos base que ser√°n adaptados a LSPy
- Validaci√≥n de t√©cnicas de preprocesamiento y extracci√≥n de features
- Transfer learning para acelerar el entrenamiento de modelos LSPy

El sistema utiliza **MediaPipe** para extracci√≥n de landmarks y modelos **CNN-LSTM** optimizados para alcanzar alta precisi√≥n en tiempo real.

## üéØ Caracter√≠sticas Principales

### Sistema Unificado de Reconocimiento en Tiempo Real

**Estado Actual (usando dataset LIBRAS para desarrollo):**

- ‚úÖ **Reconocimiento de Alfabeto**: 26 letras (A-Z) con 45.6% de precisi√≥n
- ‚úÖ **Handshapes por Orientaci√≥n**: 4 modelos especializados (back, front, left, right) con 100 clases cada uno
- ‚úÖ **Detecci√≥n de Ambas Manos**: Soporte simult√°neo para mano izquierda y derecha
- ‚úÖ **Traducci√≥n multiling√ºe**: Modelo transformer para conversi√≥n texto-glosas
  - Actualmente: PT-BR ‚Üí LIBRAS (modelo base)
  - Objetivo: Espa√±ol/Guaran√≠ ‚Üí LSPy
- ‚úÖ **UI Optimizada**: Interfaz mejorada con mejor contraste y visualizaci√≥n clara
- ‚úÖ **Barras de Confianza**: Visualizaci√≥n en tiempo real de la confianza de predicciones

**Pr√≥ximos Pasos para LSPy:**
- üîÑ Recolecci√≥n de dataset LSPy (letras, palabras, frases)
- üîÑ Entrenamiento de modelos espec√≠ficos para LSPy
- üîÑ Implementaci√≥n de traducci√≥n Espa√±ol ‚Üí LSPy
- üîÑ Implementaci√≥n de traducci√≥n Guaran√≠ ‚Üí LSPy
- üîÑ Letra √ë para alfabeto paraguayo

### Demo en Tiempo Real

```bash
# Ejecutar sistema completo con c√°mara (actualmente con modelos LIBRAS)
python scripts/demo_realtime_improved.py

# Controles:
# Q - Salir
# T - Traducir texto (PT-BR ‚Üí LIBRAS, futuro: ES/GN ‚Üí LSPy)
# L - Activar/desactivar visualizaci√≥n de landmarks
```

## Dependencias

Este proyecto utiliza las siguientes dependencias principales:
- Python 3.8 o superior
- TensorFlow
- MediaPipe
- NumPy
- OpenCV

Para instalar todas las dependencias, ejecute:
```bash
pip install -r requirements.txt
```

## Estructura del Proyecto

La estructura principal del proyecto es la siguiente:

```
Nembogueta---An-lisis-y-Entrenamiento/
‚îú‚îÄ‚îÄ data/                # Datos crudos y procesados
‚îú‚îÄ‚îÄ docs/                # Documentaci√≥n del proyecto
‚îú‚îÄ‚îÄ models/              # Modelos entrenados
‚îú‚îÄ‚îÄ notebooks/           # Jupyter notebooks para experimentaci√≥n
‚îú‚îÄ‚îÄ scripts/             # Scripts principales para procesamiento y demos
‚îú‚îÄ‚îÄ src/                 # C√≥digo fuente principal
‚îú‚îÄ‚îÄ tests/               # Pruebas unitarias
‚îî‚îÄ‚îÄ README.md            # Documentaci√≥n principal
```

## üöÄ Inicio R√°pido

### 1. Instalaci√≥n

```bash
# Clonar el repositorio
git clone <repository-url>
cd Nembogueta---An-lisis-y-Entrenamiento

# Crear entorno virtual
python -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Ejecutar Demo en Tiempo Real

```bash
# Demo mejorado con todas las funcionalidades
python scripts/demo_realtime_improved.py

# Opciones disponibles:
python scripts/demo_realtime_improved.py --help

# Especificar c√°mara y resoluci√≥n
python scripts/demo_realtime_improved.py --camera 0 --width 1280 --height 720
```

### 3. Entrenar Modelos

```bash
# Entrenar modelo de alfabeto (A-Z)
python scripts/train_alphabet.py \
  --data-dir data/processed/alphabet-combined \
  --output-dir data/models/alphabet \
  --epochs 50 --batch-size 64

# Entrenar modelo de handshapes
python scripts/train_handshape.py \
  --data-dir data/processed/lswh100 \
  --output-dir data/models/handshape \
  --epochs 100 --batch-size 32

# Entrenar modelo de traducci√≥n
python scripts/train_translation.py \
  --data-dir data/processed/pt_br2libras \
  --output-dir data/models/translation \
  --epochs 30 --batch-size 32

# Entrenar modelo de V-LIBRASIL
python scripts/train_vlibrasil.py \
  --data-dir data/processed/v-librasil-flat \
  --output-dir data/models/vlibrasil \
  --epochs 100 --batch-size 32
```

## üìä Modelos y Rendimiento

### Alfabeto (Dactilolog√≠a)

- **Arquitectura**: CNN-LSTM
- **Clases**: 26 letras (A-Z)
- **Muestras**: 2,748
- **Precisi√≥n**: 45.6% (validaci√≥n)
- **Features**: 63 (21 landmarks √ó 3 coordenadas)

### Handshapes

- **Arquitectura**: Dense Neural Network
- **Modelos**: 4 (por orientaci√≥n: back, front, left, right)
- **Clases por modelo**: 100
- **Precisi√≥n**: ~74% (por orientaci√≥n)
- **Features**: 63 (21 landmarks √ó 3 coordenadas)

### Traducci√≥n PT-BR ‚Üí LIBRAS

- **Arquitectura**: Transformer (Encoder-Decoder)
- **Vocabulario PT-BR**: Variable
- **Vocabulario LIBRAS**: Glosas
- **Precisi√≥n**: >99.9% (validaci√≥n)
- **Max sequence length**: 100 tokens

### V-LIBRASIL

- **Dataset**: Videos de LIBRAS
- **Arquitectura**: LSTM
- **Estado**: Modelo base entrenado

## üõ†Ô∏è Scripts Disponibles

### Preprocesamiento

```bash
# Alfabeto
python scripts/preprocess_alphabet.py \
  --data-dir data/raw/alphabet \
  --output-dir data/processed/alphabet

# Handshapes
python scripts/preprocess_lswh100.py \
  --data-dir data/raw/lswh100 \
  --output-dir data/processed/lswh100

# V-LIBRASIL
python scripts/preprocess_vlibrasil.py \
  --data-dir "data/raw/videos UFPE (V-LIBRASIL)/data" \
  --output-dir data/processed/v-librasil-flat
```

### Evaluaci√≥n

```bash
# Evaluar modelo de alfabeto
python scripts/evaluate_alphabet.py \
  --model-path data/models/alphabet/best_model.keras \
  --test-data data/processed/alphabet-combined

# Evaluar handshapes
python scripts/evaluate_handshape.py \
  --model-dir data/models/handshape \
  --test-data data/processed/lswh100

# Evaluar traducci√≥n
python scripts/evaluate_translation.py \
  --model-path data/models/translation/best_model.keras
```

### Inferencia

```bash
# Inferencia en video individual
python scripts/inference_alphabet.py \
  --model-path data/models/alphabet/best_model.keras \
  --video-path path/to/video.mp4

# Tiempo real con c√°mara
python scripts/realtime_alphabet.py \
  --model-path data/models/alphabet/best_model.keras \
  --camera 0
```

## üé® Sistema Unificado de Predicci√≥n

### Clase `LibrasUnifiedPredictor`

Predictor centralizado que carga y gestiona todos los modelos:

```python
from src.libras_unified_predictor import LibrasUnifiedPredictor

# Inicializar predictor
predictor = LibrasUnifiedPredictor(models_dir="data/models")

# Obtener predicciones desde un frame
predictions = predictor.predict_from_frame(frame, draw_landmarks=True)

# Resultados incluyen:
# - hands: Lista de predicciones por cada mano detectada
#   - handedness: "Left" o "Right"
#   - orientation: "back", "front", "left", "right"
#   - alphabet: Letra predicha con confianza
#   - handshape: Forma de mano predicha con confianza
# - facial_expression: Expresi√≥n facial (si disponible)
# - landmarks_detected: Estado de detecci√≥n

# Traducir texto PT-BR a glosas LIBRAS
glosas = predictor.translate_text_to_gloss("ol√° mundo")
# Resultado: ['OLA', 'MUNDO']
```

### Caracter√≠sticas del Predictor

- **Detecci√≥n autom√°tica de orientaci√≥n**: Clasifica la orientaci√≥n de la mano
- **M√∫ltiples manos**: Soporta detecci√≥n de mano izquierda y derecha simult√°neamente
- **Modelos especializados**: Usa el modelo de handshape apropiado seg√∫n orientaci√≥n
- **MediaPipe integrado**: Extracci√≥n autom√°tica de landmarks
- **Visualizaci√≥n opcional**: Dibuja landmarks sobre el frame

## üì∏ Demo en Tiempo Real - Caracter√≠sticas

### UI Mejorada

- **Fondos semi-transparentes**: Mejor legibilidad sin ocultar el video
- **Paneles por mano**: Informaci√≥n separada para cada mano detectada
- **Colores distintivos**: Naranja (mano derecha), Azul (mano izquierda)
- **Barras de confianza**: Visualizaci√≥n gr√°fica de certeza de predicciones
- **Controles claros**: Instrucciones siempre visibles

### Informaci√≥n Mostrada

Para cada mano detectada:
- Tipo de mano (Izquierda/Derecha)
- Orientaci√≥n (back/front/left/right)
- Letra del alfabeto con barra de confianza
- Handshape con barra de confianza

Adicional:
- Expresi√≥n facial (si disponible)
- Traducci√≥n PT-BR ‚Üí LIBRAS (al presionar T)
- FPS y rendimiento

## üê≥ Ejecuci√≥n en Docker

### Iniciar Contenedor

```bash
# Iniciar con GPU
docker compose --profile gpu up -d nembogueta-gpu

# Verificar estado
docker ps
```

### Ejecutar Scripts en Contenedor

```bash
# Entrenar modelo de alfabeto
docker exec nembogueta-dev-gpu python scripts/train_alphabet.py \
  --data-dir /app/data/processed/alphabet-combined \
  --output-dir /app/data/models/alphabet \
  --epochs 50

# Demo en tiempo real (requiere X11 forwarding)
docker exec nembogueta-dev-gpu python scripts/demo_realtime_improved.py
```

## üîß Soluci√≥n de Problemas

### Error: "No module named sklearn"

```bash
pip install scikit-learn
```

### Error: "No se detecta la c√°mara"

```bash
# Verificar c√°maras disponibles
python -c "import cv2; print(cv2.VideoCapture(0).isOpened())"

# Probar con ID diferente
python scripts/demo_realtime_improved.py --camera 1
```

### Predicciones con baja confianza

- Aseg√∫rate de tener buena iluminaci√≥n
- Mant√©n las manos visibles y dentro del frame
- Evita fondos complejos o con movimiento
- Ajusta la posici√≥n para que MediaPipe detecte correctamente

### Rendimiento lento

- Usa `--width 640 --height 480` para menor resoluci√≥n
- Desactiva landmarks con `L` durante la ejecuci√≥n
- Considera usar GPU si est√° disponible

## üìà Hoja de Ruta - LSPy (Lenguaje de Se√±as Paraguayo)

### Fase 1: Infraestructura y Modelos Base (Actual)
- [x] Sistema de preprocesamiento universal
- [x] Arquitectura CNN-LSTM para reconocimiento
- [x] Predictor unificado multi-modelo
- [x] UI en tiempo real con detecci√≥n de m√∫ltiples manos
- [x] Modelos base entrenados con LIBRAS

### Fase 2: Recolecci√≥n de Datos LSPy
- [ ] **Alfabeto LSPy** (A-Z, √ë)
  - [ ] Recolecci√≥n de videos para 27 letras
  - [ ] 10 videos por letra m√≠nimo
  - [ ] M√∫ltiples personas para diversidad
- [ ] **Palabras legales** (jur√≠dicas)
  - [ ] Juicio, abogado, fiscal, defensor, etc.
  - [ ] T√©rminos espec√≠ficos del sistema judicial paraguayo
- [ ] **Frases completas**
  - [ ] "Acceso a la justicia"
  - [ ] Frases comunes en contexto legal
  - [ ] Frases en espa√±ol y guaran√≠

### Fase 3: Entrenamiento LSPy
- [ ] Transfer learning desde modelos LIBRAS a LSPy
- [ ] Entrenamiento de alfabeto LSPy (incluyendo √ë)
- [ ] Entrenamiento de palabras jur√≠dicas
- [ ] Entrenamiento de frases completas
- [ ] Fine-tuning para espa√±ol y guaran√≠

### Fase 4: Traducci√≥n Biling√ºe
- [ ] **Espa√±ol ‚Üí LSPy**
  - [ ] Dataset de traducci√≥n Espa√±ol-Glosas LSPy
  - [ ] Modelo transformer Espa√±ol ‚Üí LSPy
- [ ] **Guaran√≠ ‚Üí LSPy**
  - [ ] Dataset de traducci√≥n Guaran√≠-Glosas LSPy
  - [ ] Modelo transformer Guaran√≠ ‚Üí LSPy
- [ ] Sistema unificado biling√ºe

### Fase 5: Optimizaci√≥n y Despliegue
- [ ] Optimizaci√≥n de modelos para edge devices
- [ ] Conversi√≥n a TensorFlow Lite
- [ ] API REST para integraci√≥n
- [ ] App m√≥vil Android/iOS
- [ ] Integraci√≥n con sistema judicial paraguayo

### Fase 6: Expansi√≥n
- [ ] Entrenamiento de modelo de expresiones faciales
- [ ] Reconocimiento de contexto y gram√°tica LSPy
- [ ] Soporte para m√°s dominios (educaci√≥n, salud, etc.)
- [ ] Sistema de retroalimentaci√≥n y mejora continua

## üéØ Objetivos del Proyecto

Este proyecto busca:

1. **Democratizar el acceso a la justicia** en Paraguay mediante tecnolog√≠a de reconocimiento de se√±as
2. **Preservar y promover** el Lenguaje de Se√±as Paraguayo (LSPy)
3. **Facilitar la comunicaci√≥n** entre personas sordas y el sistema judicial
4. **Apoyar el biling√ºismo** paraguayo (Espa√±ol y Guaran√≠) en el contexto de LSPy
5. **Crear herramientas de c√≥digo abierto** para la comunidad sorda paraguaya

## ü§ù Contribuci√≥n

1. Sigue las directrices de calidad de c√≥digo (Black, isort, Flake8)
2. Agrega pruebas unitarias para nueva funcionalidad
3. Actualiza la documentaci√≥n seg√∫n sea necesario
4. Env√≠a pull requests para revisi√≥n

### C√≥mo Contribuir con Datos LSPy

Si eres hablante de LSPy y quieres contribuir:
- Contacta al equipo para participar en recolecci√≥n de videos
- Ayuda a validar las se√±as reconocidas
- Proporciona feedback sobre la precisi√≥n del sistema

## üìù Licencia

[Especificar licencia]

## üìß Contacto

[Especificar informaci√≥n de contacto]

## üôè Agradecimientos

- Comunidad sorda paraguaya
- Dataset V-LIBRASIL por proporcionar data base para desarrollo
- Proyecto MediaPipe de Google por la tecnolog√≠a de landmarks
- Comunidad de c√≥digo abierto

---

**Desarrollado con ‚ù§Ô∏è para la comunidad sorda paraguaya**
**√ëemongeta - Hablemos en se√±as**
